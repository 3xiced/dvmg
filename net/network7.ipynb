{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import app.dvmg.patterns\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get patterns list\n",
    "patterns_list: list = inspect.getmembers(\n",
    "        app.dvmg.patterns, inspect.isclass)\n",
    "f_patterns_list: list = [\n",
    "    ptrn for ptrn in patterns_list if ptrn[0] != 'PatternBase' and ptrn[0] != 'Custom']\n",
    "\n",
    "print(f_patterns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataSet\n",
    "\n",
    "onlyfiles = [f for f in listdir(\"../dataset/\") if isfile(join(\"../dataset/\", f))]\n",
    "\n",
    "# Create empty dataset lists\n",
    "dataset_model_1: list[tuple[tuple, str]] = list() # ((X,y), pattern_name)\n",
    "test_dataset_model_1: list[tuple[tuple, str]] = list() # ((test_data_X,test_data_y), pattern_name)\n",
    "\n",
    "dataset_model_2: list[tuple[np.ndarray, int]] = list() # (X, class_id)\n",
    "test_dataset_model_2: list[tuple[np.ndarray, int]] = list() # (X, class_id)\n",
    "\n",
    "# Paths to data files\n",
    "training_file_path = f'../dataset/{onlyfiles[0]}'\n",
    "test_file_path = f'../dataset/{onlyfiles[1]}'\n",
    "\n",
    "def parse(path: str, pattern_name: str) -> tuple[np.ndarray, np.ndarray, list]:\n",
    "\n",
    "    X_model_2: list[tuple[np.ndarray, int]] = list() # Данные для обучения для полносвязной НС\n",
    "    X_model_1 = np.empty((0, 14), int) # Данные для обучения (координаты реконструкции фазового портрета) до сортировки для перцептронов\n",
    "    y_model_1 = np.array([]) # 1 - соответствует правильному паттерну, на который тренируется сеть, 0 - всем остальным до сортировки для перцептронов\n",
    "\n",
    "    random_value = np.array([]) # Случайная величина для перемешивания датасета\n",
    "    with open(path, 'r') as json_file:\n",
    "        data: dict = json.load(json_file)\n",
    "        \n",
    "        for ptrn in list(data.keys()):\n",
    "            all_coordinates: list = data[ptrn]\n",
    "            \n",
    "            for local_cordinates in all_coordinates:\n",
    "                X_model_2.append((np.array([np.append(np.array(local_cordinates['x']), np.array(local_cordinates['y'])).tolist()]), list(data.keys()).index(ptrn)))\n",
    "                X_model_1 = np.append(X_model_1, np.array([np.append(np.array(local_cordinates['x']), np.array(local_cordinates['y'])).tolist()]), axis=0)\n",
    "                y_model_1 = np.append(y_model_1, 1) if ptrn == pattern_name else np.append(y_model_1, 0)\n",
    "                random_value = np.append(random_value, np.random.rand()) # Генерация случайного числа от 0 до 1\n",
    "\n",
    "    # Сортировка по случайным величинам\n",
    "    return { #type: ignore\n",
    "        \"X_model_1\": np.array([x for _, x, _ in sorted(zip(random_value, X_model_1, y_model_1), key=lambda x: x[0])]),\n",
    "        \"y_model_1\": np.array([y for _, _, y in sorted(zip(random_value, X_model_1, y_model_1), key=lambda x: x[0])]),\n",
    "        \"X_model_2\": [x for _, x in sorted(zip(random_value, X_model_2), key=lambda x: x[0])]\n",
    "    }\n",
    "\n",
    "\n",
    "for ptrn in f_patterns_list:\n",
    "\n",
    "    pattern_name, signature = ptrn\n",
    "    data = parse(training_file_path, pattern_name)\n",
    "\n",
    "    X_model_1 = data[\"X_model_1\"] #type: ignore\n",
    "    y_model_1 = data[\"y_model_1\"] #type: ignore\n",
    "    X_model_2 = data[\"X_model_2\"] #type: ignore\n",
    "\n",
    "    dataset_model_1 += [((X_model_1, y_model_1), pattern_name)]\n",
    "    dataset_model_2 = [*dataset_model_2, *X_model_2]\n",
    "\n",
    "    test_data = parse(test_file_path, pattern_name)\n",
    "\n",
    "    X_model_1 = test_data[\"X_model_1\"] #type: ignore\n",
    "    y_model_1 = test_data[\"y_model_1\"] #type: ignore \n",
    "    X_model_2 = test_data[\"X_model_2\"] #type: ignore\n",
    "\n",
    "    test_dataset_model_1 += [((X_model_1, y_model_1), pattern_name)]\n",
    "    test_dataset_model_2 = [*test_dataset_model_2, *X_model_2]\n",
    "\n",
    "print(dataset_model_1[0])\n",
    "print('\\n-----------------\\n')\n",
    "print(dataset_model_2[0])\n",
    "print('\\n-----------------\\n')\n",
    "\n",
    "print(test_dataset_model_1[0])\n",
    "print('\\n-----------------\\n')\n",
    "print(test_dataset_model_2[0])\n",
    "print('\\n-----------------\\n')\n",
    "\n",
    "print(len(dataset_model_1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "INPUT_DIM = 14\n",
    "OUT_DIM = 7\n",
    "H_DIM = 16\n",
    "\n",
    "_dataset = dataset_model_2\n",
    "\n",
    "def relu(t):\n",
    "    return np.maximum(t, 0)\n",
    "\n",
    "def softmax(t):\n",
    "    out = np.exp(t)\n",
    "    return out / np.sum(out)\n",
    "\n",
    "def softmax_batch(t):\n",
    "    out = np.exp(t)\n",
    "    return out / np.sum(out, axis=1, keepdims=True)\n",
    "\n",
    "def sparse_cross_entropy(z, y):\n",
    "    return -np.log(z[0, y])\n",
    "\n",
    "def sparse_cross_entropy_batch(z, y):\n",
    "    return -np.log(np.array([z[j, y[j]] for j in range(len(y))]))\n",
    "\n",
    "def to_full(y, num_classes):\n",
    "    y_full = np.zeros((1, num_classes))\n",
    "    y_full[0, y] = 1\n",
    "    return y_full\n",
    "\n",
    "def to_full_batch(y, num_classes):\n",
    "    y_full = np.zeros((len(y), num_classes))\n",
    "    for j, yj in enumerate(y):\n",
    "        y_full[j, yj] = 1\n",
    "    return y_full\n",
    "\n",
    "def relu_deriv(t):\n",
    "    return (t >= 0).astype(float)\n",
    "\n",
    "W1 = np.random.rand(INPUT_DIM, H_DIM)\n",
    "b1 = np.random.rand(1, H_DIM)\n",
    "W2 = np.random.rand(H_DIM, OUT_DIM)\n",
    "b2 = np.random.rand(1, OUT_DIM)\n",
    "\n",
    "W1 = (W1 - 0.5) * 2 * np.sqrt(1/INPUT_DIM)\n",
    "b1 = (b1 - 0.5) * 2 * np.sqrt(1/INPUT_DIM)\n",
    "W2 = (W2 - 0.5) * 2 * np.sqrt(1/H_DIM)\n",
    "b2 = (b2 - 0.5) * 2 * np.sqrt(1/H_DIM)\n",
    "\n",
    "ALPHA = 0.0001\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "loss_arr = []\n",
    "\n",
    "for ep in range(NUM_EPOCHS):\n",
    "    random.shuffle(_dataset)\n",
    "    print(f'[Epoch]: {ep}')\n",
    "    for i in range(len(_dataset) // BATCH_SIZE):\n",
    "\n",
    "        batch_x, batch_y = zip(*_dataset[i*BATCH_SIZE : i*BATCH_SIZE+BATCH_SIZE])\n",
    "        x = np.concatenate(batch_x, axis=0)\n",
    "        y = np.array(batch_y)\n",
    "\n",
    "        # Forward\n",
    "        t1 = x @ W1 + b1\n",
    "        h1 = relu(t1)\n",
    "        t2 = h1 @ W2 + b2\n",
    "        z = softmax_batch(t2)\n",
    "        E = np.sum(sparse_cross_entropy_batch(z, y))\n",
    "\n",
    "        # Backward\n",
    "        y_full = to_full_batch(y, OUT_DIM)\n",
    "        dE_dt2 = z - y_full\n",
    "        dE_dW2 = h1.T @ dE_dt2\n",
    "        dE_db2 = np.sum(dE_dt2, axis=0, keepdims=True)\n",
    "        dE_dh1 = dE_dt2 @ W2.T\n",
    "        dE_dt1 = dE_dh1 * relu_deriv(t1)\n",
    "        dE_dW1 = x.T @ dE_dt1\n",
    "        dE_db1 = np.sum(dE_dt1, axis=0, keepdims=True)\n",
    "\n",
    "        # Update\n",
    "        W1 = W1 - ALPHA * dE_dW1\n",
    "        b1 = b1 - ALPHA * dE_db1\n",
    "        W2 = W2 - ALPHA * dE_dW2\n",
    "        b2 = b2 - ALPHA * dE_db2\n",
    "\n",
    "        loss_arr.append(E)\n",
    "\n",
    "def predict(x):\n",
    "    t1 = x @ W1 + b1\n",
    "    h1 = relu(t1)\n",
    "    t2 = h1 @ W2 + b2\n",
    "    z = softmax_batch(t2)\n",
    "    return z\n",
    "\n",
    "def calc_accuracy():\n",
    "    correct = 0\n",
    "    for x, y in _dataset:\n",
    "        z = predict(x)\n",
    "        y_pred = np.argmax(z)\n",
    "        if y_pred == y:\n",
    "            correct += 1\n",
    "    acc = correct / len(_dataset)\n",
    "    return acc\n",
    "\n",
    "accuracy = calc_accuracy()\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_arr)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_dataset = test_dataset_model_2\n",
    "random.shuffle(_test_dataset)\n",
    "\n",
    "counter = 0\n",
    "for data in _test_dataset:\n",
    "    x, y = data\n",
    "    z = predict(x[0])\n",
    "    y_pred = np.argmax(z)\n",
    "    if y_pred == y:\n",
    "        counter += 1\n",
    "print(\"Test accuracy: \" + str(counter/len(_test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, lr=0.01, epochs=5):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Our fit function trains on the dataset X and tries to predict vector y,\n",
    "        Using the learning rate, it will modify it's weight vector to increase\n",
    "        it's accuracy in predictions.\n",
    "        It will iterate over the X dataset as defined by the epochs.\n",
    "        Args:\n",
    "            X: The input data (numpy array of shape [n_samples * m_features])\n",
    "            y: Class labels vector (numpy array of shape [n_samples])\n",
    "        \"\"\"\n",
    "        # a vector of floats between 0 and 1\n",
    "        weights = np.random.rand(X.shape[1],)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # list of predicted classes for our accuracy calculation\n",
    "            predicted = []\n",
    "            for i_index, sample in enumerate(X):\n",
    "                y_hat = self.predict(sample, weights)\n",
    "                predicted.append(y_hat)  # add our new prediction to the array\n",
    "                for j_index, feature in enumerate(weights):\n",
    "                    # update our weight values\n",
    "                    delta = self.lr * (y[i_index] - y_hat)\n",
    "                    delta = delta * sample[j_index-1]\n",
    "                    weights[j_index-1] = weights[j_index-1] + delta\n",
    "            # print('[Epoch {ep}] Accuracy: {acc}'.format(\n",
    "            #     ep=epoch, acc=self._calculate_accuracy(y, predicted)\n",
    "            # ))\n",
    "        self.weights = weights\n",
    "\n",
    "    def _calculate_accuracy(self, actual, predicted):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of predictions for this epoch.\n",
    "        Args:\n",
    "            actual: vector of actual class values (the y vector) [n_samples]\n",
    "            predicted: vector of predicted class values [n_samples]\n",
    "        \"\"\"\n",
    "        return sum(np.array(predicted) == np.array(actual)) / float(len(actual))\n",
    "\n",
    "    def predict(self, x, w):\n",
    "        \"\"\"\n",
    "        Create a binary prediction from an activation function on the data\n",
    "        sample and the weight vector.\n",
    "        Args:\n",
    "            x: vector of the data sample - shape [m_features]\n",
    "            w: vector of the weights - shape [m_features]\n",
    "        Returns:\n",
    "            0 or 1\n",
    "        \"\"\"\n",
    "        res = self._sum(x, w)\n",
    "        # print(res)\n",
    "        return 1 if res > 0.0 else 0.0\n",
    "\n",
    "    def _sum(self, x, w):\n",
    "        \"\"\"\n",
    "        Multiply our sample and weight vector elements then the sum of the\n",
    "        result.\n",
    "        Args:\n",
    "            x: vector of the data sample - shape [m_features]\n",
    "            w: vector of the weights - shape [m_features]\n",
    "        Returns:\n",
    "            Int of the sum of vector products\n",
    "        \"\"\"\n",
    "        return np.sum(np.dot(x, np.transpose(w)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create perceptron instances and train\n",
    "\n",
    "perceptrons: list[Perceptron] = [Perceptron() for _ in range(len(f_patterns_list))]\n",
    "perceptrons_patterns_names: list = list()\n",
    "\n",
    "k = 0\n",
    "for data in dataset_model_1:\n",
    "    (X, y), name = data\n",
    "    perceptrons[k].fit(X, y)\n",
    "    perceptrons_patterns_names += [name]\n",
    "    k += 1\n",
    "\n",
    "[print(perceptrons[p].weights, perceptrons_patterns_names[p]) for p in range(len(perceptrons))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test linked perceptrons with softmax\n",
    "\n",
    "test_data_ = test_dataset_model_2\n",
    "\n",
    "print(perceptrons_patterns_names)\n",
    "\n",
    "new_dataset_model_2: list[tuple[np.ndarray, int]] = list() # (X, class_id)\n",
    "print(len(test_data_))\n",
    "for t_p_t in test_data_:\n",
    "    t_t, tr = t_p_t\n",
    "    true_answer = perceptrons_patterns_names[tr]\n",
    "    perceptrons_predictions: list = []\n",
    "    for perc in perceptrons:\n",
    "        perceptrons_predictions += [perc._sum(t_t[0], perc.weights)]\n",
    "    new_dataset_model_2.append((np.array([perceptrons_predictions]), perceptrons_patterns_names.index(true_answer)))\n",
    "\n",
    "len(new_dataset_model_2)\n",
    "new_dataset_model_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нейронная сеть соединяющая выводы перцептронов\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "INPUT_DIM = 7\n",
    "OUT_DIM = 7\n",
    "H_DIM = 12\n",
    "\n",
    "_dataset = new_dataset_model_2\n",
    "\n",
    "def relu(t):\n",
    "    return np.maximum(t, 0)\n",
    "\n",
    "def softmax(t):\n",
    "    out = np.exp(t)\n",
    "    return out / np.sum(out)\n",
    "\n",
    "def softmax_batch(t):\n",
    "    out = np.exp(t)\n",
    "    return out / np.sum(out, axis=1, keepdims=True)\n",
    "\n",
    "def sparse_cross_entropy(z, y):\n",
    "    return -np.log(z[0, y])\n",
    "\n",
    "def sparse_cross_entropy_batch(z, y):\n",
    "    return -np.log(np.array([z[j, y[j]] for j in range(len(y))]))\n",
    "\n",
    "def to_full(y, num_classes):\n",
    "    y_full = np.zeros((1, num_classes))\n",
    "    y_full[0, y] = 1\n",
    "    return y_full\n",
    "\n",
    "def to_full_batch(y, num_classes):\n",
    "    y_full = np.zeros((len(y), num_classes))\n",
    "    for j, yj in enumerate(y):\n",
    "        y_full[j, yj] = 1\n",
    "    return y_full\n",
    "\n",
    "def relu_deriv(t):\n",
    "    return (t >= 0).astype(float)\n",
    "\n",
    "W1 = np.random.rand(INPUT_DIM, H_DIM)\n",
    "b1 = np.random.rand(1, H_DIM)\n",
    "W2 = np.random.rand(H_DIM, OUT_DIM)\n",
    "b2 = np.random.rand(1, OUT_DIM)\n",
    "\n",
    "W1 = (W1 - 0.5) * 2 * np.sqrt(1/INPUT_DIM)\n",
    "b1 = (b1 - 0.5) * 2 * np.sqrt(1/INPUT_DIM)\n",
    "W2 = (W2 - 0.5) * 2 * np.sqrt(1/H_DIM)\n",
    "b2 = (b2 - 0.5) * 2 * np.sqrt(1/H_DIM)\n",
    "\n",
    "ALPHA = 0.0001\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "loss_arr = []\n",
    "\n",
    "for ep in range(NUM_EPOCHS):\n",
    "    random.shuffle(_dataset)\n",
    "    print(f'[Epoch]: {ep}')\n",
    "    for i in range(len(_dataset) // BATCH_SIZE):\n",
    "\n",
    "        batch_x, batch_y = zip(*_dataset[i*BATCH_SIZE : i*BATCH_SIZE+BATCH_SIZE])\n",
    "        x = np.concatenate(batch_x, axis=0)\n",
    "        y = np.array(batch_y)\n",
    "\n",
    "        # Forward\n",
    "        t1 = x @ W1 + b1\n",
    "        h1 = relu(t1)\n",
    "        t2 = h1 @ W2 + b2\n",
    "        z = softmax_batch(t2)\n",
    "        E = np.sum(sparse_cross_entropy_batch(z, y))\n",
    "\n",
    "        # Backward\n",
    "        y_full = to_full_batch(y, OUT_DIM)\n",
    "        dE_dt2 = z - y_full\n",
    "        dE_dW2 = h1.T @ dE_dt2\n",
    "        dE_db2 = np.sum(dE_dt2, axis=0, keepdims=True)\n",
    "        dE_dh1 = dE_dt2 @ W2.T\n",
    "        dE_dt1 = dE_dh1 * relu_deriv(t1)\n",
    "        dE_dW1 = x.T @ dE_dt1\n",
    "        dE_db1 = np.sum(dE_dt1, axis=0, keepdims=True)\n",
    "\n",
    "        # Update\n",
    "        W1 = W1 - ALPHA * dE_dW1\n",
    "        b1 = b1 - ALPHA * dE_db1\n",
    "        W2 = W2 - ALPHA * dE_dW2\n",
    "        b2 = b2 - ALPHA * dE_db2\n",
    "\n",
    "        loss_arr.append(E)\n",
    "\n",
    "def predict(x):\n",
    "    t1 = x @ W1 + b1\n",
    "    h1 = relu(t1)\n",
    "    t2 = h1 @ W2 + b2\n",
    "    z = softmax_batch(t2)\n",
    "    return z\n",
    "\n",
    "def calc_accuracy():\n",
    "    correct = 0\n",
    "    for x, y in _dataset:\n",
    "        z = predict(x)\n",
    "        y_pred = np.argmax(z)\n",
    "        if y_pred == y:\n",
    "            correct += 1\n",
    "    acc = correct / len(_dataset)\n",
    "    return acc\n",
    "\n",
    "accuracy = calc_accuracy()\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_arr)\n",
    "plt.show()\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5102983151a502f0b3c21dd441eb79f35619ea9e057c95bc886ff9f2b76b13c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
