{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import app.dvmg.patterns\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LinearDecrease', <class 'app.dvmg.patterns.builtin.LinearDecrease'>), ('LinearIncrease', <class 'app.dvmg.patterns.builtin.LinearIncrease'>), ('Normal', <class 'app.dvmg.patterns.builtin.Normal'>), ('NormalFlipped', <class 'app.dvmg.patterns.builtin.NormalFlipped'>), ('Plain', <class 'app.dvmg.patterns.builtin.Plain'>), ('Sigmoid', <class 'app.dvmg.patterns.builtin.Sigmoid'>), ('SigmoidReversed', <class 'app.dvmg.patterns.builtin.SigmoidReversed'>)]\n"
     ]
    }
   ],
   "source": [
    "# Get patterns list\n",
    "patterns_list: list = inspect.getmembers(\n",
    "        app.dvmg.patterns, inspect.isclass)\n",
    "f_patterns_list: list = [\n",
    "    ptrn for ptrn in patterns_list if ptrn[0] != 'PatternBase' and ptrn[0] != 'Custom']\n",
    "\n",
    "print(f_patterns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 16 and the array at index 1 has size 14",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mfor\u001b[39;00m ptrn \u001b[39min\u001b[39;00m f_patterns_list:\n\u001b[0;32m     45\u001b[0m     pattern_name, signature \u001b[39m=\u001b[39m ptrn\n\u001b[1;32m---> 46\u001b[0m     data \u001b[39m=\u001b[39m parse(training_file_path, pattern_name)\n\u001b[0;32m     48\u001b[0m     X_model_1 \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mX_model_1\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     49\u001b[0m     y_model_1 \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39my_model_1\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(path, pattern_name)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mfor\u001b[39;00m local_cordinates \u001b[39min\u001b[39;00m all_coordinates:\n\u001b[0;32m     30\u001b[0m     X_model_2\u001b[39m.\u001b[39mappend((np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39marray(local_cordinates[\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m]), np\u001b[39m.\u001b[39marray(local_cordinates[\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m]))\u001b[39m.\u001b[39mtolist()]), \u001b[39mlist\u001b[39m(data\u001b[39m.\u001b[39mkeys())\u001b[39m.\u001b[39mindex(ptrn)))\n\u001b[1;32m---> 31\u001b[0m     X_model_1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mappend(X_model_1, np\u001b[39m.\u001b[39;49marray([np\u001b[39m.\u001b[39;49mappend(np\u001b[39m.\u001b[39;49marray(local_cordinates[\u001b[39m'\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m'\u001b[39;49m]), np\u001b[39m.\u001b[39;49marray(local_cordinates[\u001b[39m'\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m'\u001b[39;49m]))\u001b[39m.\u001b[39;49mtolist()]), axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     32\u001b[0m     y_model_1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(y_model_1, \u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m ptrn \u001b[39m==\u001b[39m pattern_name \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mappend(y_model_1, \u001b[39m0\u001b[39m)\n\u001b[0;32m     33\u001b[0m     random_value \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(random_value, np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand()) \u001b[39m# Генерация случайного числа от 0 до 1\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\o3xic\\Projects\\DeepViewModelsGenerator\\env\\Lib\\site-packages\\numpy\\lib\\function_base.py:5444\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     values \u001b[39m=\u001b[39m ravel(values)\n\u001b[0;32m   5443\u001b[0m     axis \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mndim\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 5444\u001b[0m \u001b[39mreturn\u001b[39;00m concatenate((arr, values), axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 16 and the array at index 1 has size 14"
     ]
    }
   ],
   "source": [
    "# Create DataSet\n",
    "\n",
    "onlyfiles = [f for f in listdir(\"../dataset/\") if isfile(join(\"../dataset/\", f))]\n",
    "\n",
    "# Create empty dataset lists\n",
    "dataset_model_1: list[tuple[tuple, str]] = list() # ((X,y), pattern_name)\n",
    "test_dataset_model_1: list[tuple[tuple, str]] = list() # ((test_data_X,test_data_y), pattern_name)\n",
    "\n",
    "dataset_model_2: list[tuple[np.ndarray, int]] = list() # (X, class_id)\n",
    "test_dataset_model_2: list[tuple[np.ndarray, int]] = list() # (X, class_id)\n",
    "\n",
    "# Paths to data files\n",
    "training_file_path = f'../dataset/{onlyfiles[0]}'\n",
    "test_file_path = f'../dataset/{onlyfiles[1]}'\n",
    "\n",
    "def parse(path: str, pattern_name: str) -> tuple[np.ndarray, np.ndarray, list]:\n",
    "\n",
    "    X_model_2: list[tuple[np.ndarray, int]] = list() # Данные для обучения для полносвязной НС\n",
    "    X_model_1 = np.empty((0, 16), int) # Данные для обучения (координаты реконструкции фазового портрета) до сортировки для перцептронов\n",
    "    y_model_1 = np.array([]) # 1 - соответствует правильному паттерну, на который тренируется сеть, 0 - всем остальным до сортировки для перцептронов\n",
    "\n",
    "    random_value = np.array([]) # Случайная величина для перемешивания датасета\n",
    "    with open(path, 'r') as json_file:\n",
    "        data: dict = json.load(json_file)\n",
    "        \n",
    "        for ptrn in list(data.keys()):\n",
    "            all_coordinates: list = data[ptrn]\n",
    "            \n",
    "            for local_cordinates in all_coordinates:\n",
    "                X_model_2.append((np.array([np.append(np.array(local_cordinates['x']), np.array(local_cordinates['y'])).tolist()]), list(data.keys()).index(ptrn)))\n",
    "                X_model_1 = np.append(X_model_1, np.array([np.append(np.array(local_cordinates['x']), np.array(local_cordinates['y'])).tolist()]), axis=0)\n",
    "                y_model_1 = np.append(y_model_1, 1) if ptrn == pattern_name else np.append(y_model_1, 0)\n",
    "                random_value = np.append(random_value, np.random.rand()) # Генерация случайного числа от 0 до 1\n",
    "\n",
    "    # Сортировка по случайным величинам\n",
    "    return {\n",
    "        \"X_model_1\": np.array([x for _, x, _ in sorted(zip(random_value, X_model_1, y_model_1), key=lambda x: x[0])]),\n",
    "        \"y_model_1\": np.array([y for _, _, y in sorted(zip(random_value, X_model_1, y_model_1), key=lambda x: x[0])]),\n",
    "        \"X_model_2\": [x for _, x in sorted(zip(random_value, X_model_2), key=lambda x: x[0])]\n",
    "    }\n",
    "\n",
    "\n",
    "for ptrn in f_patterns_list:\n",
    "\n",
    "    pattern_name, signature = ptrn\n",
    "    data = parse(training_file_path, pattern_name)\n",
    "\n",
    "    X_model_1 = data[\"X_model_1\"]\n",
    "    y_model_1 = data[\"y_model_1\"]\n",
    "    X_model_2 = data[\"X_model_2\"]\n",
    "\n",
    "    dataset_model_1 += [((X_model_1, y_model_1), pattern_name)]\n",
    "    dataset_model_2 = [*dataset_model_2, *X_model_2]\n",
    "\n",
    "    test_data = parse(test_file_path, pattern_name)\n",
    "\n",
    "    X_model_1 = test_data[\"X_model_1\"]\n",
    "    y_model_1 = test_data[\"y_model_1\"]\n",
    "    X_model_2 = test_data[\"X_model_2\"]\n",
    "\n",
    "    test_dataset_model_1 += [((X_model_1, y_model_1), pattern_name)]\n",
    "    test_dataset_model_2 = [*test_dataset_model_2, *X_model_2]\n",
    "\n",
    "print(dataset_model_1[0])\n",
    "print('\\n-----------------\\n')\n",
    "print(dataset_model_2[0])\n",
    "print('\\n-----------------\\n')\n",
    "\n",
    "print(test_dataset_model_1[0])\n",
    "print('\\n-----------------\\n')\n",
    "print(test_dataset_model_2[0])\n",
    "print('\\n-----------------\\n')\n",
    "\n",
    "print(len(dataset_model_1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "INPUT_DIM = 16\n",
    "OUT_DIM = 7\n",
    "H_DIM = 20\n",
    "\n",
    "_dataset = dataset_model_2\n",
    "\n",
    "def relu(t):\n",
    "    return np.maximum(t, 0)\n",
    "\n",
    "def softmax(t):\n",
    "    out = np.exp(t)\n",
    "    return out / np.sum(out)\n",
    "\n",
    "def softmax_batch(t):\n",
    "    out = np.exp(t)\n",
    "    return out / np.sum(out, axis=1, keepdims=True)\n",
    "\n",
    "def sparse_cross_entropy(z, y):\n",
    "    return -np.log(z[0, y])\n",
    "\n",
    "def sparse_cross_entropy_batch(z, y):\n",
    "    return -np.log(np.array([z[j, y[j]] for j in range(len(y))]))\n",
    "\n",
    "def to_full(y, num_classes):\n",
    "    y_full = np.zeros((1, num_classes))\n",
    "    y_full[0, y] = 1\n",
    "    return y_full\n",
    "\n",
    "def to_full_batch(y, num_classes):\n",
    "    y_full = np.zeros((len(y), num_classes))\n",
    "    for j, yj in enumerate(y):\n",
    "        y_full[j, yj] = 1\n",
    "    return y_full\n",
    "\n",
    "def relu_deriv(t):\n",
    "    return (t >= 0).astype(float)\n",
    "\n",
    "W1 = np.random.rand(INPUT_DIM, H_DIM)\n",
    "b1 = np.random.rand(1, H_DIM)\n",
    "W2 = np.random.rand(H_DIM, OUT_DIM)\n",
    "b2 = np.random.rand(1, OUT_DIM)\n",
    "\n",
    "W1 = (W1 - 0.5) * 2 * np.sqrt(1/INPUT_DIM)\n",
    "b1 = (b1 - 0.5) * 2 * np.sqrt(1/INPUT_DIM)\n",
    "W2 = (W2 - 0.5) * 2 * np.sqrt(1/H_DIM)\n",
    "b2 = (b2 - 0.5) * 2 * np.sqrt(1/H_DIM)\n",
    "\n",
    "ALPHA = 0.0001\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "loss_arr = []\n",
    "\n",
    "for ep in range(NUM_EPOCHS):\n",
    "    random.shuffle(_dataset)\n",
    "    print(f'[Epoch]: {ep}')\n",
    "    for i in range(len(_dataset) // BATCH_SIZE):\n",
    "\n",
    "        batch_x, batch_y = zip(*_dataset[i*BATCH_SIZE : i*BATCH_SIZE+BATCH_SIZE])\n",
    "        x = np.concatenate(batch_x, axis=0)\n",
    "        y = np.array(batch_y)\n",
    "\n",
    "        # Forward\n",
    "        t1 = x @ W1 + b1\n",
    "        h1 = relu(t1)\n",
    "        t2 = h1 @ W2 + b2\n",
    "        z = softmax_batch(t2)\n",
    "        E = np.sum(sparse_cross_entropy_batch(z, y))\n",
    "\n",
    "        # Backward\n",
    "        y_full = to_full_batch(y, OUT_DIM)\n",
    "        dE_dt2 = z - y_full\n",
    "        dE_dW2 = h1.T @ dE_dt2\n",
    "        dE_db2 = np.sum(dE_dt2, axis=0, keepdims=True)\n",
    "        dE_dh1 = dE_dt2 @ W2.T\n",
    "        dE_dt1 = dE_dh1 * relu_deriv(t1)\n",
    "        dE_dW1 = x.T @ dE_dt1\n",
    "        dE_db1 = np.sum(dE_dt1, axis=0, keepdims=True)\n",
    "\n",
    "        # Update\n",
    "        W1 = W1 - ALPHA * dE_dW1\n",
    "        b1 = b1 - ALPHA * dE_db1\n",
    "        W2 = W2 - ALPHA * dE_dW2\n",
    "        b2 = b2 - ALPHA * dE_db2\n",
    "\n",
    "        loss_arr.append(E)\n",
    "\n",
    "def predict(x):\n",
    "    t1 = x @ W1 + b1\n",
    "    h1 = relu(t1)\n",
    "    t2 = h1 @ W2 + b2\n",
    "    z = softmax_batch(t2)\n",
    "    return z\n",
    "\n",
    "def calc_accuracy():\n",
    "    correct = 0\n",
    "    for x, y in _dataset:\n",
    "        z = predict(x)\n",
    "        y_pred = np.argmax(z)\n",
    "        if y_pred == y:\n",
    "            correct += 1\n",
    "    acc = correct / len(_dataset)\n",
    "    return acc\n",
    "\n",
    "accuracy = calc_accuracy()\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_arr)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_dataset = test_dataset_model_2\n",
    "random.shuffle(_test_dataset)\n",
    "\n",
    "counter = 0\n",
    "for data in _test_dataset:\n",
    "    x, y = data\n",
    "    z = predict(x[0])\n",
    "    y_pred = np.argmax(z)\n",
    "    if y_pred == y:\n",
    "        counter += 1\n",
    "print(\"Test accuracy: \" + str(counter/len(_test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, lr=0.01, epochs=5):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Our fit function trains on the dataset X and tries to predict vector y,\n",
    "        Using the learning rate, it will modify it's weight vector to increase\n",
    "        it's accuracy in predictions.\n",
    "        It will iterate over the X dataset as defined by the epochs.\n",
    "        Args:\n",
    "            X: The input data (numpy array of shape [n_samples * m_features])\n",
    "            y: Class labels vector (numpy array of shape [n_samples])\n",
    "        \"\"\"\n",
    "        # a vector of floats between 0 and 1\n",
    "        weights = np.random.rand(X.shape[1],)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # list of predicted classes for our accuracy calculation\n",
    "            predicted = []\n",
    "            for i_index, sample in enumerate(X):\n",
    "                y_hat = self.predict(sample, weights)\n",
    "                predicted.append(y_hat)  # add our new prediction to the array\n",
    "                for j_index, feature in enumerate(weights):\n",
    "                    # update our weight values\n",
    "                    delta = self.lr * (y[i_index] - y_hat)\n",
    "                    delta = delta * sample[j_index-1]\n",
    "                    weights[j_index-1] = weights[j_index-1] + delta\n",
    "            # print('[Epoch {ep}] Accuracy: {acc}'.format(\n",
    "            #     ep=epoch, acc=self._calculate_accuracy(y, predicted)\n",
    "            # ))\n",
    "        self.weights = weights\n",
    "\n",
    "    def _calculate_accuracy(self, actual, predicted):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of predictions for this epoch.\n",
    "        Args:\n",
    "            actual: vector of actual class values (the y vector) [n_samples]\n",
    "            predicted: vector of predicted class values [n_samples]\n",
    "        \"\"\"\n",
    "        return sum(np.array(predicted) == np.array(actual)) / float(len(actual))\n",
    "\n",
    "    def predict(self, x, w):\n",
    "        \"\"\"\n",
    "        Create a binary prediction from an activation function on the data\n",
    "        sample and the weight vector.\n",
    "        Args:\n",
    "            x: vector of the data sample - shape [m_features]\n",
    "            w: vector of the weights - shape [m_features]\n",
    "        Returns:\n",
    "            0 or 1\n",
    "        \"\"\"\n",
    "        res = self._sum(x, w)\n",
    "        # print(res)\n",
    "        return 1 if res > 0.0 else 0.0\n",
    "\n",
    "    def _sum(self, x, w):\n",
    "        \"\"\"\n",
    "        Multiply our sample and weight vector elements then the sum of the\n",
    "        result.\n",
    "        Args:\n",
    "            x: vector of the data sample - shape [m_features]\n",
    "            w: vector of the weights - shape [m_features]\n",
    "        Returns:\n",
    "            Int of the sum of vector products\n",
    "        \"\"\"\n",
    "        return np.sum(np.dot(x, np.transpose(w)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create perceptron instances and train\n",
    "\n",
    "perceptrons: list[Perceptron] = [Perceptron() for _ in range(len(f_patterns_list))]\n",
    "perceptrons_patterns_names: list = list()\n",
    "\n",
    "k = 0\n",
    "for data in dataset_model_1:\n",
    "    (X, y), name = data\n",
    "    perceptrons[k].fit(X, y)\n",
    "    perceptrons_patterns_names += [name]\n",
    "    k += 1\n",
    "\n",
    "[print(perceptrons[p].weights, perceptrons_patterns_names[p]) for p in range(len(perceptrons))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test linked perceptrons with softmax\n",
    "\n",
    "test_data_ = test_dataset_model_2\n",
    "\n",
    "print(perceptrons_patterns_names)\n",
    "\n",
    "new_dataset_model_2: list[tuple[np.ndarray, int]] = list() # (X, class_id)\n",
    "print(len(test_data_))\n",
    "for t_p_t in test_data_:\n",
    "    t_t, tr = t_p_t\n",
    "    true_answer = perceptrons_patterns_names[tr]\n",
    "    perceptrons_predictions: list = []\n",
    "    for perc in perceptrons:\n",
    "        perceptrons_predictions += [perc._sum(t_t[0], perc.weights)]\n",
    "    new_dataset_model_2.append((np.array([perceptrons_predictions]), perceptrons_patterns_names.index(true_answer)))\n",
    "\n",
    "len(new_dataset_model_2)\n",
    "new_dataset_model_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нейронная сеть соединяющая выводы перцептронов\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "INPUT_DIM = 7\n",
    "OUT_DIM = 7\n",
    "H_DIM = 12\n",
    "\n",
    "_dataset = new_dataset_model_2\n",
    "\n",
    "def relu(t):\n",
    "    return np.maximum(t, 0)\n",
    "\n",
    "def softmax(t):\n",
    "    out = np.exp(t)\n",
    "    return out / np.sum(out)\n",
    "\n",
    "def softmax_batch(t):\n",
    "    out = np.exp(t)\n",
    "    return out / np.sum(out, axis=1, keepdims=True)\n",
    "\n",
    "def sparse_cross_entropy(z, y):\n",
    "    return -np.log(z[0, y])\n",
    "\n",
    "def sparse_cross_entropy_batch(z, y):\n",
    "    return -np.log(np.array([z[j, y[j]] for j in range(len(y))]))\n",
    "\n",
    "def to_full(y, num_classes):\n",
    "    y_full = np.zeros((1, num_classes))\n",
    "    y_full[0, y] = 1\n",
    "    return y_full\n",
    "\n",
    "def to_full_batch(y, num_classes):\n",
    "    y_full = np.zeros((len(y), num_classes))\n",
    "    for j, yj in enumerate(y):\n",
    "        y_full[j, yj] = 1\n",
    "    return y_full\n",
    "\n",
    "def relu_deriv(t):\n",
    "    return (t >= 0).astype(float)\n",
    "\n",
    "W1 = np.random.rand(INPUT_DIM, H_DIM)\n",
    "b1 = np.random.rand(1, H_DIM)\n",
    "W2 = np.random.rand(H_DIM, OUT_DIM)\n",
    "b2 = np.random.rand(1, OUT_DIM)\n",
    "\n",
    "W1 = (W1 - 0.5) * 2 * np.sqrt(1/INPUT_DIM)\n",
    "b1 = (b1 - 0.5) * 2 * np.sqrt(1/INPUT_DIM)\n",
    "W2 = (W2 - 0.5) * 2 * np.sqrt(1/H_DIM)\n",
    "b2 = (b2 - 0.5) * 2 * np.sqrt(1/H_DIM)\n",
    "\n",
    "ALPHA = 0.0001\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "loss_arr = []\n",
    "\n",
    "for ep in range(NUM_EPOCHS):\n",
    "    random.shuffle(_dataset)\n",
    "    print(f'[Epoch]: {ep}')\n",
    "    for i in range(len(_dataset) // BATCH_SIZE):\n",
    "\n",
    "        batch_x, batch_y = zip(*_dataset[i*BATCH_SIZE : i*BATCH_SIZE+BATCH_SIZE])\n",
    "        x = np.concatenate(batch_x, axis=0)\n",
    "        y = np.array(batch_y)\n",
    "\n",
    "        # Forward\n",
    "        t1 = x @ W1 + b1\n",
    "        h1 = relu(t1)\n",
    "        t2 = h1 @ W2 + b2\n",
    "        z = softmax_batch(t2)\n",
    "        E = np.sum(sparse_cross_entropy_batch(z, y))\n",
    "\n",
    "        # Backward\n",
    "        y_full = to_full_batch(y, OUT_DIM)\n",
    "        dE_dt2 = z - y_full\n",
    "        dE_dW2 = h1.T @ dE_dt2\n",
    "        dE_db2 = np.sum(dE_dt2, axis=0, keepdims=True)\n",
    "        dE_dh1 = dE_dt2 @ W2.T\n",
    "        dE_dt1 = dE_dh1 * relu_deriv(t1)\n",
    "        dE_dW1 = x.T @ dE_dt1\n",
    "        dE_db1 = np.sum(dE_dt1, axis=0, keepdims=True)\n",
    "\n",
    "        # Update\n",
    "        W1 = W1 - ALPHA * dE_dW1\n",
    "        b1 = b1 - ALPHA * dE_db1\n",
    "        W2 = W2 - ALPHA * dE_dW2\n",
    "        b2 = b2 - ALPHA * dE_db2\n",
    "\n",
    "        loss_arr.append(E)\n",
    "\n",
    "def predict(x):\n",
    "    t1 = x @ W1 + b1\n",
    "    h1 = relu(t1)\n",
    "    t2 = h1 @ W2 + b2\n",
    "    z = softmax_batch(t2)\n",
    "    return z\n",
    "\n",
    "def calc_accuracy():\n",
    "    correct = 0\n",
    "    for x, y in _dataset:\n",
    "        z = predict(x)\n",
    "        y_pred = np.argmax(z)\n",
    "        if y_pred == y:\n",
    "            correct += 1\n",
    "    acc = correct / len(_dataset)\n",
    "    return acc\n",
    "\n",
    "accuracy = calc_accuracy()\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_arr)\n",
    "plt.show()\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5102983151a502f0b3c21dd441eb79f35619ea9e057c95bc886ff9f2b76b13c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
