{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import app.dvmg.patterns\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import inspect\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get patterns list\n",
    "patterns_list: list = inspect.getmembers(\n",
    "        app.dvmg.patterns, inspect.isclass)\n",
    "f_patterns_list: list = [\n",
    "    ptrn for ptrn in patterns_list if ptrn[0] != 'PatternBase' and ptrn[0] != 'Custom']\n",
    "\n",
    "print(f_patterns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataSet\n",
    "\n",
    "onlyfiles = [f for f in listdir(\"../dataset/\") if isfile(join(\"../dataset/\", f))]\n",
    "\n",
    "# Create empty dataset lists\n",
    "dataset_model_1: list[tuple[tuple, str]] = list() # ((X,y), pattern_name)\n",
    "test_dataset_model_1: list[tuple[tuple, str]] = list() # ((test_data_X,test_data_y), pattern_name)\n",
    "\n",
    "dataset_model_2: list[tuple[np.ndarray, int]] = list() # (X, class_id)\n",
    "test_dataset_model_2: list[tuple[np.ndarray, int]] = list() # (X, class_id)\n",
    "\n",
    "# Paths to data files\n",
    "training_file_path = f'../dataset/{onlyfiles[0]}'\n",
    "test_file_path = f'../dataset/{onlyfiles[1]}'\n",
    "\n",
    "def parse(path: str, pattern_name: str) -> tuple[np.ndarray, np.ndarray, list]:\n",
    "\n",
    "    X_model_2: list[tuple[np.ndarray, list]] = list() # Данные для обучения для полносвязной НС\n",
    "    X_model_1 = np.empty((0, 14), int) # Данные для обучения (координаты реконструкции фазового портрета) до сортировки для перцептронов\n",
    "    y_model_1 = np.array([]) # 1 - соответствует правильному паттерну, на который тренируется сеть, 0 - всем остальным до сортировки для перцептронов\n",
    "\n",
    "    random_value = np.array([]) # Случайная величина для перемешивания датасета\n",
    "    with open(path, 'r') as json_file:\n",
    "        data: dict = json.load(json_file)\n",
    "        \n",
    "        for ptrn in list(data.keys()):\n",
    "            all_coordinates: list = data[ptrn]\n",
    "            \n",
    "            for local_cordinates in all_coordinates:\n",
    "                X_model_2.append(([np.append(np.array(local_cordinates['x']), np.array(local_cordinates['y'])).tolist()], list(data.keys()).index(ptrn))) # type: ignore\n",
    "                X_model_1 = np.append(X_model_1, np.array([np.append(np.array(local_cordinates['x']), np.array(local_cordinates['y'])).tolist()]), axis=0)\n",
    "                y_model_1 = np.append(y_model_1, 1) if ptrn == pattern_name else np.append(y_model_1, 0)\n",
    "                random_value = np.append(random_value, np.random.rand()) # Генерация случайного числа от 0 до 1\n",
    "\n",
    "    # Сортировка по случайным величинам\n",
    "    return { # type: ignore\n",
    "        \"X_model_1\": np.array([x for _, x, _ in sorted(zip(random_value, X_model_1, y_model_1), key=lambda x: x[0])]),\n",
    "        \"y_model_1\": np.array([y for _, _, y in sorted(zip(random_value, X_model_1, y_model_1), key=lambda x: x[0])]),\n",
    "        \"X_model_2\": [x for _, x in sorted(zip(random_value, X_model_2), key=lambda x: x[0])]\n",
    "    }\n",
    "\n",
    "\n",
    "for ptrn in f_patterns_list:\n",
    "\n",
    "    pattern_name, signature = ptrn\n",
    "    data = parse(training_file_path, pattern_name)\n",
    "\n",
    "    X_model_1 = data[\"X_model_1\"] # type: ignore\n",
    "    y_model_1 = data[\"y_model_1\"] # type: ignore\n",
    "    X_model_2 = data[\"X_model_2\"] # type: ignore\n",
    "\n",
    "    dataset_model_1 += [((X_model_1, y_model_1), pattern_name)]\n",
    "    dataset_model_2 = [*dataset_model_2, *X_model_2]\n",
    "\n",
    "    test_data = parse(test_file_path, pattern_name)\n",
    "\n",
    "    X_model_1 = test_data[\"X_model_1\"] # type: ignore\n",
    "    y_model_1 = test_data[\"y_model_1\"] # type: ignore\n",
    "    X_model_2 = test_data[\"X_model_2\"] # type: ignore\n",
    "\n",
    "    test_dataset_model_1 += [((X_model_1, y_model_1), pattern_name)]\n",
    "    test_dataset_model_2 = [*test_dataset_model_2, *X_model_2]\n",
    "\n",
    "# print(dataset_model_1[0])\n",
    "# print('\\n-----------------\\n')\n",
    "print(dataset_model_2[0])\n",
    "print('\\n-----------------\\n')\n",
    "\n",
    "# print(test_dataset_model_1[0])\n",
    "# print('\\n-----------------\\n')\n",
    "# print(test_dataset_model_2[0])\n",
    "# print('\\n-----------------\\n')\n",
    "\n",
    "# print(len(dataset_model_1))\n",
    "\n",
    "x_train = list()\n",
    "y_train = list()\n",
    "\n",
    "for sample in dataset_model_2:\n",
    "    x_train.append(sample[0][0])\n",
    "    y_train.append(sample[1])\n",
    "\n",
    "x_train = tf.convert_to_tensor(x_train)\n",
    "y_train = to_categorical(y_train, 7)\n",
    "\n",
    "print(x_train, y_train)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(14,)))\n",
    "model.add(Dense(48, activation='sigmoid', name='hidden_1'))\n",
    "model.add(Dense(24, activation='relu', name='hidden_2'))\n",
    "model.add(Dense(12, activation='relu', name='hidden_3'))\n",
    "model.add(Dense(7, activation='softmax', name='output'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=20, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from app.utils import *\n",
    "\n",
    "lines: list = open('./data/cameraDetections.txt').read().splitlines()\n",
    "\n",
    "timestamps: list = list()\n",
    "\n",
    "for line in lines:\n",
    "    if '2021-' in line:\n",
    "        continue\n",
    "    if '+04' in line:\n",
    "        line = line.replace('+04', '')\n",
    "    if '.' not in line:\n",
    "        line += '.0'\n",
    "    time = datetime.strptime(line, '%Y-%m-%d %H:%M:%S.%f') - timedelta(hours=1)\n",
    "    timestamps.append(time.timestamp())\n",
    "\n",
    "dt_strings: list[str] = list()\n",
    "dt_objects: list[datetime] = list()\n",
    "for tmsp in timestamps:\n",
    "    dt_strings += [datetime.fromtimestamp(tmsp).strftime('%Y-%m-%d %H:%M:%S.%f')]\n",
    "    dt_objects += [datetime.fromtimestamp(tmsp)]\n",
    "\n",
    "DEVIDER = 100\n",
    "\n",
    "counter = 0\n",
    "start_time = dt_objects[0] - timedelta(minutes=15)\n",
    "end_time = start_time + timedelta(hours=12)\n",
    "while end_time < dt_objects[-1]:\n",
    "    counter += 1\n",
    "    start_time = start_time + timedelta(minutes=15)\n",
    "    end_time = start_time + timedelta(hours=12)\n",
    "\n",
    "    to_inspect_timestamps: list[float] = [dt.timestamp() for dt in dt_objects if dt > start_time and dt <= end_time]\n",
    "    to_inspect_timestamps_dt: list[str] = [dt.strftime('%Y-%m-%d %H:%M:%S.%f') for dt in dt_objects if dt > start_time and dt <= end_time]\n",
    "    # print(to_inspect_timestamps_dt)\n",
    "\n",
    "    output: dict = dict()\n",
    "    cut_timestamp = [to_inspect_timestamps[i] - min(to_inspect_timestamps) + .001 for i in range(len(to_inspect_timestamps))]\n",
    "    if len(cut_timestamp) == 0:\n",
    "        continue\n",
    "\n",
    "    for time in cut_timestamp:\n",
    "        output[time] = 1\n",
    "    # print(list(output.keys()))\n",
    "    to_hist = process_coordinates_to_histogram(list(output.keys()), DEVIDER)\n",
    "    x, y = compile_phase_portrait(to_hist, DEVIDER, 20)\n",
    "    rx, ry = compile_phase_reconstruction_quantile(x, y)\n",
    "    test_data = tf.convert_to_tensor(np.array(rx + ry), tf.float32)\n",
    "\n",
    "    # Predict and print\n",
    "    prediction = model.predict(tf.expand_dims(test_data, axis=0)) #type: ignore\n",
    "    tt = start_time.strftime('%Y-%m-%d %H:%M:%S').replace(':','-')\n",
    "    class_name = f_patterns_list[prediction.tolist()[0].index(max(prediction.tolist()[0]))][0]\n",
    "    print(f\"ITER [{counter}] TIME: {tt} \" + class_name)\n",
    "    plt.close('all')\n",
    "    plt.rcParams[\"figure.figsize\"] = [20, 2]\n",
    "    plt.bar(list(output.keys()), list(output.values()), width=50)  # type: ignore\n",
    "    plt.yticks([1])\n",
    "    plt.xlabel(\"время, c\")\n",
    "    plt.ylabel(\"событие\")\n",
    "    plt.savefig(f'./images/events/{tt}+{class_name}.png')\n",
    "\n",
    "\n",
    "# timestamps = timestamps[66:120] #78 #169\n",
    "# timestamps = timestamps[78:190] #78 #169\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5102983151a502f0b3c21dd441eb79f35619ea9e057c95bc886ff9f2b76b13c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
